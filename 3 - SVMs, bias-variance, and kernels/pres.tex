\documentclass{beamer}

\mode<presentation>
{
\usetheme{Copenhagen}
%\usetheme{Boadilla}
%\usecolortheme{seahorse}
%\useoutertheme{infolines}
%\useoutertheme[compress]{miniframes}
%\setbeamercovered{transparent}
}

%% \mode<presentation>
%% {
%% \usetheme{progressbar}
%% \setbeamercovered{transparent}
%% }

%\usepackage[english]{babel}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{mathptmx}
\usepackage[scaled=.90]{helvet}
\usepackage[T1]{fontenc}
\usepackage{xspace}
%\usepackage{appendixnumberbeamer}
\usepackage[noend]{algorithmic}
%\usepackage[algo2e,vlined,algochapter,ruled,dotocloa]{algorithm2e}
\usepackage{fancybox}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{amssymb,amsmath}
% \usepackage{ulem}
\usepackage{makecell}
\usepackage{times}
\usepackage{bbding}

\title[A few topics in Reinforcement Learning]{From Field problems to Machine Learning}
\subtitle{An introduction to the Data Science workflow\\and a motivation to understand Machine Learning}
%\title{The Optimal Swapping Problem during Nuclear Refueling Operations}


\author{E. Rachelson}

\institute{\includegraphics[width=1.5cm]{img/isae.jpg}}

\date{}


% This is only inserted into the PDF information catalog. Can be left
% out.
\subject{From Predictive Maintenance to Machine Learning}

\setbeamerfont{bibliography entry author}{shape=\upshape,series=\bfseries,size=\footnotesize}%
\setbeamerfont{bibliography entry title}{shape=\upshape,size=\scriptsize,series=\mdseries}
\setbeamerfont{bibliography entry journal}{shape=\upshape,size=\scriptsize,series=\mdseries}
\setbeamerfont{bibliography entry note}{shape=\upshape,size=\scriptsize,series=\mdseries}

\setbeamercolor{block}{bg=blue,fg=red}

\beamertemplatenavigationsymbolsempty
\setbeamertemplate{footline}[frame number]

\begin{document}

\begin{frame}{A geometrical approach to ML}
\begin{block}{}
\begin{enumerate}
\item Draw a line that sits as far as possible from the data points $\rightarrow$ Support Vector Machines
\item Send all data points in a higher dimension space where they are linearly separable $\rightarrow$ kernel trick
\end{enumerate}
$\Rightarrow$ SVM + kernel trick = Find the optimal separating hyperplane in this higher dimension space, without ever computing the mapping.
\end{block}

\begin{itemize}
\item SVM try to separate data by maximizing a geometrical margin
\item They are computed offline
\item They offer a sparse, robust to class imbalance, and easy to evaluate predictor
\item Kernels are a way of enriching (lifting) the data representation so that it becomes linearly separable
\item SVMs + kernels offer a versatile method for classification, regression and density estimation
\item \href{http://scikit-learn.org/stable/modules/svm.html}{[Link] to documentation in scikit-learn}
\end{itemize}
\end{frame}

\end{document}
